{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\VDgae\\Projetos\\Nova pasta\\mammography-images\\train_test.ipynb CÃ©lula 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpydicom\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Flatten, Dropout\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageDataGenerator\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\Lib\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[39m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# now we will test our model on test data\n",
    "# first we train our model on train data without any preprocessing\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import cv2\n",
    "import pydicom\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.applications.densenet import DenseNet121\n",
    "#from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv('train_calc_annotations.csv')\n",
    "test = pd.read_csv('test_calc_annotations.csv')\n",
    "\n",
    "# create train and test data\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    # load dicom images\n",
    "    image = pydicom.dcmread(train['cropped_file'][i]).pixel_array\n",
    "    # resize images\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # normalize images\n",
    "    image = image / 255.0\n",
    "    # append data\n",
    "    train_data.append(image)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    # load dicom images\n",
    "    image = pydicom.dcmread(test['cropped_file'][i]).pixel_array\n",
    "    # resize images\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # normalize images\n",
    "    image = image / 255.0\n",
    "    # append data\n",
    "    test_data.append(image)\n",
    "\n",
    "# convert data into numpy array\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "# create train and test labels\n",
    "train_labels = train['pathology'].values\n",
    "test_labels = test['pathology'].values\n",
    "\n",
    "# convert labels into categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# create model\n",
    "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train model\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels))\n",
    "\n",
    "# save model\n",
    "model.save('model.h5')\n",
    "\n",
    "# save history\n",
    "with open('history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "\n",
    "# plot accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "# plot loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "# predict test data\n",
    "predictions = model.predict(test_data)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# create confusion matrix\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# create classification report\n",
    "print(classification_report(test_labels, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
