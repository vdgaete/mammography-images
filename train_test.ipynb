{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded...\n",
      "labels converted...\n",
      "Data Format:\n",
      "(1446, 224, 224, 3)\n",
      "(1446, 2)\n",
      "(100, 224, 224, 3)\n",
      "(100, 2)\n",
      "Epoch 1/20\n",
      "46/46 [==============================] - 149s 3s/step - loss: 35.5482 - accuracy: 0.6093 - val_loss: 6.5549 - val_accuracy: 0.6400 - lr: 0.0030\n",
      "Epoch 2/20\n",
      "46/46 [==============================] - 155s 3s/step - loss: 4.0773 - accuracy: 0.6432 - val_loss: 0.4962 - val_accuracy: 0.6700 - lr: 0.0030\n",
      "Epoch 3/20\n",
      "46/46 [==============================] - 154s 3s/step - loss: 0.7411 - accuracy: 0.6680 - val_loss: 0.4999 - val_accuracy: 0.6700 - lr: 0.0030\n",
      "Epoch 4/20\n",
      "12/46 [======>.......................] - ETA: 1:43 - loss: 0.6555 - accuracy: 0.6899"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\VDgae\\Projetos\\Nova pasta\\mammography-images\\train_test.ipynb CÃ©lula 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m datagen \u001b[39m=\u001b[39m ImageDataGenerator(horizontal_flip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, vertical_flip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rotation_range\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, zoom_range\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m                              width_shift_range\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, height_shift_range\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, shear_range\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(datagen\u001b[39m.\u001b[39;49mflow(train_data, train_labels, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m), epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m                               validation_data\u001b[39m=\u001b[39;49m(val_data, val_labels), callbacks\u001b[39m=\u001b[39;49m[early_stopping, reduce_lr])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39m# save model\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/VDgae/Projetos/Nova%20pasta/mammography-images/train_test.ipynb#W0sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mresnet50.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\VDgae\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now we will test our model on test data\n",
    "# first we train our model on train data without any preprocessing\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import cv2\n",
    "import pydicom\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv('train_calc_annotations.csv')\n",
    "test = pd.read_csv('test_calc_annotations.csv')\n",
    "\n",
    "# create train and test data\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "# print\n",
    "print('Loading data...')\n",
    "\n",
    "for i in range(len(train)):\n",
    "    # load dicom images\n",
    "    image = pydicom.dcmread(train['cropped_file'][i]).pixel_array\n",
    "    # resize images\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # normalize images\n",
    "    image = image / 255.0\n",
    "    # make 3 channels\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "    # append data\n",
    "    train_data.append(image)\n",
    "\n",
    "for i in range(len(test)):\n",
    "    # load dicom images\n",
    "    image = pydicom.dcmread(test['cropped_file'][i]).pixel_array\n",
    "    # resize images\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    # normalize images\n",
    "    image = image / 255.0\n",
    "    # make 3 channels\n",
    "    image = np.stack((image,)*3, axis=-1)\n",
    "    # append data\n",
    "    test_data.append(image)\n",
    "\n",
    "print('Data loaded...')\n",
    "\n",
    "# convert data into numpy array\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "# create train and test labels\n",
    "train_labels = train['pathology'].values\n",
    "test_labels = test['pathology'].values\n",
    "\n",
    "# list possible labels\n",
    "labels = ['BENIGN', 'MALIGNANT']\n",
    "\n",
    "# if label is BENIGN_WITHOUT_CALLBACK then convert it into BENIGN\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i] == 'BENIGN_WITHOUT_CALLBACK':\n",
    "        train_labels[i] = 'BENIGN'\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == 'BENIGN_WITHOUT_CALLBACK':\n",
    "        test_labels[i] = 'BENIGN'\n",
    "\n",
    "\n",
    "# convert labels into int values\n",
    "for i in range(len(train_labels)):\n",
    "    if train_labels[i] == 'BENIGN':\n",
    "        train_labels[i] = 0\n",
    "    else:\n",
    "        train_labels[i] = 1\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == 'BENIGN':\n",
    "        test_labels[i] = 0\n",
    "    else:\n",
    "        test_labels[i] = 1\n",
    "\n",
    "    \n",
    "\n",
    "# convert labels into categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(\"labels converted...\")\n",
    "\n",
    "# create internal validation data\n",
    "val_data = train_data[:100]\n",
    "val_labels = train_labels[:100]\n",
    "\n",
    "# create train data\n",
    "train_data = train_data[100:]\n",
    "train_labels = train_labels[100:]\n",
    "\n",
    "print('Data Format:')\n",
    "# print shapes\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train resnet50 model\n",
    "# load resnet50 model\n",
    "resnet50 = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# freeze layers\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add layers\n",
    "x = resnet50.output\n",
    "# flatten layer\n",
    "# convert 3d to 1d\n",
    "x = Flatten()(x)\n",
    "# add dense layer\n",
    "# 512 neurons\n",
    "# relu activation\n",
    "# dense layer is fully connected layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "# add dropout\n",
    "# to avoid overfitting\n",
    "x = Dropout(0.5)(x)\n",
    "# output layer\n",
    "# 3 neurons\n",
    "# softmax activation\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "# each neuron will give probability of each class\n",
    "# class with highest probability will be the output\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=resnet50.input, outputs=output)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.003), metrics=['accuracy'])\n",
    "\n",
    "# create callbacks\\\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=30, zoom_range=0.2,\n",
    "                             width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(datagen.flow(train_data, train_labels, batch_size=32), epochs=20,\n",
    "                              validation_data=(val_data, val_labels), callbacks=[early_stopping, reduce_lr])\n",
    "# save model\n",
    "model.save('resnet50.h5')\n",
    "\n",
    "# plot accuracy and loss\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "# predict test data\n",
    "predictions = model.predict(test_data)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(np.argmax(test_labels, axis=1), predictions))\n",
    "\n",
    "# print confusion matrix\n",
    "print(confusion_matrix(np.argmax(test_labels, axis=1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vgg16 model\n",
    "# load vgg16 model\n",
    "vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# freeze layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add layers\n",
    "x = vgg16.output\n",
    "# flatten layer\n",
    "x = Flatten()(x)\n",
    "# add dense layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "# add dropout\n",
    "x = Dropout(0.5)(x)\n",
    "# output layer\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=vgg16.input, outputs=output)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# load model\n",
    "model = load_model('vgg16.h5')\n",
    "\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=30, zoom_range=0.2,\n",
    "                             width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(datagen.flow(train_data, train_labels, batch_size=32), epochs=20,\n",
    "                              validation_data=(val_data, val_labels), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# save model\n",
    "model.save('vgg16.h5')\n",
    "\n",
    "# plot accuracy and loss\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'])\n",
    "\n",
    "# predict test data\n",
    "predictions = model.predict(test_data)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(np.argmax(test_labels, axis=1), predictions))\n",
    "\n",
    "# print confusion matrix\n",
    "print(confusion_matrix(np.argmax(test_labels, axis=1), predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train inceptionv3 model\n",
    "# load inceptionv3 model\n",
    "inceptionv3 = InceptionV3(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "# freeze layers\n",
    "for layer in inceptionv3.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# add layers\n",
    "x = inceptionv3.output\n",
    "# flatten layer\n",
    "x = Flatten()(x)\n",
    "# add dense layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "# add dropout\n",
    "x = Dropout(0.5)(x)\n",
    "# output layer\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=inceptionv3.input, outputs=output)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# load model\n",
    "\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=30, zoom_range=0.2,\n",
    "                             width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(datagen.flow(train_data, train_labels, batch_size=32), epochs=20,\n",
    "                              validation_data=(val_data, val_labels), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# save model\n",
    "model.save('inceptionv3.h5')\n",
    "\n",
    "# plot accuracy and loss\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()\n",
    "\n",
    "# predict test data\n",
    "predictions = model.predict(test_data)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(np.argmax(test_labels, axis=1), predictions))\n",
    "\n",
    "# print confusion matrix\n",
    "print(confusion_matrix(np.argmax(test_labels, axis=1), predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
